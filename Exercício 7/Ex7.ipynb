{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CI8fNbys1WIN"
   },
   "source": [
    "# SCC-ICMC-USP - 2o. semestre de 2021\n",
    "# SCC0275 - Exercício 6\n",
    "\n",
    "### Profa. Roseli A. F. Romero\n",
    "\n",
    "### Monitor: Kenzo Sakiyama\n",
    "\n",
    "Nro do grupo:\n",
    "\n",
    "Alunos:\n",
    "\n",
    "\n",
    "1.   Stefan Taiguara Couperus Leal - 10414866\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vAoYjWV851lS"
   },
   "source": [
    "## Parte 1 - Medidas de avaliação e bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ae7mLCZI-DjC"
   },
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Akk5Sv8BM3go"
   },
   "source": [
    "### Carregando dados e definindo função de preprocessamento\n",
    "\n",
    "Para as próximas questões, utilizaremos o dataset **breast_cancer** disponibilizado pelo sklearn. A descrição do conjunto de dados e de seus atributos será apresentada a seguir (atente-se ao atributo alvo \"class\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "wRDthSyd90j4",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                   0.07871  ...          17.33           184.60      2019.0   \n",
       "1                   0.05667  ...          23.41           158.80      1956.0   \n",
       "2                   0.05999  ...          25.53           152.50      1709.0   \n",
       "3                   0.09744  ...          26.50            98.87       567.7   \n",
       "4                   0.05883  ...          16.67           152.20      1575.0   \n",
       "..                      ...  ...            ...              ...         ...   \n",
       "564                 0.05623  ...          26.40           166.10      2027.0   \n",
       "565                 0.05533  ...          38.25           155.00      1731.0   \n",
       "566                 0.05648  ...          34.12           126.70      1124.0   \n",
       "567                 0.07016  ...          39.42           184.60      1821.0   \n",
       "568                 0.05884  ...          30.37            59.16       268.6   \n",
       "\n",
       "     worst smoothness  worst compactness  worst concavity  \\\n",
       "0             0.16220            0.66560           0.7119   \n",
       "1             0.12380            0.18660           0.2416   \n",
       "2             0.14440            0.42450           0.4504   \n",
       "3             0.20980            0.86630           0.6869   \n",
       "4             0.13740            0.20500           0.4000   \n",
       "..                ...                ...              ...   \n",
       "564           0.14100            0.21130           0.4107   \n",
       "565           0.11660            0.19220           0.3215   \n",
       "566           0.11390            0.30940           0.3403   \n",
       "567           0.16500            0.86810           0.9387   \n",
       "568           0.08996            0.06444           0.0000   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  class  \n",
       "0                  0.2654          0.4601                  0.11890      0  \n",
       "1                  0.1860          0.2750                  0.08902      0  \n",
       "2                  0.2430          0.3613                  0.08758      0  \n",
       "3                  0.2575          0.6638                  0.17300      0  \n",
       "4                  0.1625          0.2364                  0.07678      0  \n",
       "..                    ...             ...                      ...    ...  \n",
       "564                0.2216          0.2060                  0.07115      0  \n",
       "565                0.1628          0.2572                  0.06637      0  \n",
       "566                0.1418          0.2218                  0.07820      0  \n",
       "567                0.2650          0.4087                  0.12400      0  \n",
       "568                0.0000          0.2871                  0.07039      1  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "df = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "df[\"class\"] = cancer.target\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1626115287029,
     "user": {
      "displayName": "Kenzo Miranda Sakiyama",
      "photoUrl": "",
      "userId": "17190914206091993570"
     },
     "user_tz": 240
    },
    "id": "yoABXLh2BYOI",
    "outputId": "ed5cb0ba-eb3b-40f2-da1e-bdfcb27df4a8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(data[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "MFHj3PDfNQzJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 31)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def preprocess(x_treino, x_teste, y_treino, y_teste):\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    # Computando estatísticas dos dados de treino\n",
    "    scaler.fit(x_treino)\n",
    "    # Transformando dados com a normalização MinMax\n",
    "    x_treino_novo = scaler.transform(x_treino)\n",
    "    x_teste_novo = scaler.transform(x_teste)\n",
    "\n",
    "    # Não é necessário alterar os labels a menos que seja feito um tipo de amostragem (over/under sampling)\n",
    "    y_treino_novo = y_treino\n",
    "    y_teste_novo = y_teste\n",
    "\n",
    "    return x_treino_novo, x_teste_novo, y_treino_novo, y_teste_novo\n",
    "\n",
    "\n",
    "def accuracy(conf_mat):\n",
    "    return (sum(np.diagonal(conf_mat))) / (conf_mat.sum())\n",
    "\n",
    "\n",
    "# confusion_matrix(y_test, y_pred)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5IuqiJcd4b6D"
   },
   "source": [
    "### Questão 01. \n",
    "\n",
    "-  Implemente a técnica bootstrap utilizando a função abaixo\n",
    "- Sugestão, utilize o material de apoio como referência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "WaR7HyvO9zss"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Perceptron\n",
    "from numpy import mean, std\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def classificacao_bootstrap(data, columns, target, preproc_fn, score_fn, score_name, conf_matrix_fn, \n",
    "                   no_bs=1000, p_teste=0.2, plot=True):\n",
    "    \"\"\"\n",
    "    Executa classificação do conjunto de dados passado\n",
    "    ---------------------------------------------------------------\n",
    "    data:       DataFrame. Conjunto de dados\n",
    "    columns:    Lista de inteiros. Índice das colunas utilizadas no treinamento e teste\n",
    "    target:     Inteiro. Índice da coluna alvo\n",
    "    preproc_fn: Função. Faz o pré-processamento da base já separada em treino e teste \n",
    "    score_fn:   Função. A função que calcula a medida de desempenho desejada. Deve ser uma \n",
    "                função que compara dois vetores, o primeiro vetor são os valores preditos\n",
    "                pelo classificador, o segundo os rótulos reais\n",
    "                Vide exemplo das funções em \n",
    "                http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "                como por exemplo, sklearn.metrics.accuracy_score\n",
    "                http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\n",
    "    score_name: String. Uma string com o nome da medida de desempenho\n",
    "    conf_matrix_fn: Função. Calcula matriz de confusão \n",
    "    folds:      Inteiro. Número de folds na validação cruzada\n",
    "    plot:       Booleano. True para plotar os gráficos False para não plotar\n",
    "    ---------------------------------------------------------------\n",
    "    Realiza a classificação em 6 modelos (perceptron, \n",
    "    SVM com kernel polinomial de grau 3, Árvore de decisão, 3NN, 5NN, e 7NN)\n",
    "    Plot o gráfico de desempenho para cada classificador.\n",
    "    Retorna um dicionário com os classiff    \"\"\"\n",
    "    # inicializa os modelos com os parâmetros solicitados\n",
    "    prcp = Perceptron()\n",
    "    dt = DecisionTreeClassifier(criterion='gini', splitter='best', min_samples_split=int(len(data)*0.1))\n",
    "    \n",
    "    clfs = [prcp, dt]\n",
    "    clfs_names = ['perceptron', 'dt']\n",
    "    \n",
    "    #Inicializa estruturas para matrizes de confusão \n",
    "    confusion_matrices = {\n",
    "        'perceptron': np.zeros((2,2)),\n",
    "        'dt': np.zeros((2,2)),\n",
    "    }\n",
    "    \n",
    "    results = {'perceptron':[], 'dt':[]}\n",
    "    \n",
    "    no_exemplos = df.shape[0]\n",
    "    p_treino = 0.8\n",
    "    p_teste = 1 - p_treino\n",
    "    no_bootstrap = 1000\n",
    "\n",
    "    scores = []\n",
    "    for i in range(0, no_bootstrap):\n",
    "        amostra = data.sample(n = no_exemplos, replace=True) # Obtendo amostra do bootstrap\n",
    "\n",
    "        X = amostra.iloc[:, 0:30] # Separando em dados \n",
    "        y = amostra.iloc[:, -1]  # e variável alvo\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=p_teste) # Split dos dados de maneira estratificada (mantendo prop. das classes)\n",
    "        ss = StandardScaler()\n",
    "        X_train = ss.fit_transform(X_train)\n",
    "        X_test = ss.transform(X_test)\n",
    "\n",
    "\n",
    "        data_train = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "        data_train[\"target\"] = cancer.target\n",
    "    \n",
    "        classe_0 = data_train[data_train['target'] == 0]\n",
    "        classe_1 = data_train[data_train['target'] == 1]\n",
    "        n_elementos_classe_0 = data_train['target'].value_counts()[0] # Coletando o número de exemplos da classe 0 para superamostragem da classe 1  \n",
    "        data_train = pd.concat([classe_0, classe_1.sample(n=n_elementos_classe_0, replace=True)]) # Superamostragem da classe 1 para numero de elementos da classe 0\n",
    "\n",
    "        X_train, y_train = data_train.iloc[:, 0:30], data_train.iloc[:, -1] # reatribuição dos dados pra treino (agora superamostrados)\n",
    "\n",
    "        clfs[0].fit(X_train, y_train)  # MUDAR PRA FUNÇÂO QUE SERÁ USADA\n",
    "        y_pred = clfs[0].predict(X_test)\n",
    "        scores.append(balanced_accuracy_score(y_test, y_pred))\n",
    "   \n",
    "    if not plot:\n",
    "        return {'results': results, 'clfs':clfs}\n",
    "    # faz o plot de desempenho dos classificadores\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.bar(range(1, len(clfs)+1), [mean(results[name]) for name in clfs_names], \n",
    "                                yerr=[std(results[name]) for name in clfs_names])\n",
    "    plt.xticks(range(1, len(clfs)+1), clfs_names, rotation=45)\n",
    "    title = 'Desempenho dos classificadores - %s'%(score_name)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "    return {'results': results, 'clfs':clfs, 'confusion_matrices': confusion_matrices}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "NJWy5TLNrp6T"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'balanced_accuracy_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-adc22efd9251>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m results = classificacao_bootstrap(df, data.data, data.target, preprocess, accuracy, \"accuracy\", confusion_matrix, \n\u001b[0m\u001b[1;32m      2\u001b[0m                    no_bs=1000, p_teste=0.2, plot=True)\n",
      "\u001b[0;32m<ipython-input-21-6ab88923de29>\u001b[0m in \u001b[0;36mclassificacao_bootstrap\u001b[0;34m(data, columns, target, preproc_fn, score_fn, score_name, conf_matrix_fn, no_bs, p_teste, plot)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mclfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# MUDAR PRA FUNÇÂO QUE SERÁ USADA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbalanced_accuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'balanced_accuracy_score' is not defined"
     ]
    }
   ],
   "source": [
    "results = classificacao_bootstrap(df, data.data, data.target, preprocess, accuracy, \"accuracy\", confusion_matrix, \n",
    "                   no_bs=1000, p_teste=0.2, plot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KUBtQR7l5hUX"
   },
   "source": [
    "### Questão 02. \n",
    "\n",
    "- Rode a função 4 vezes utilizando 2 métricas diferentes do **sklearn.metrics** (2 execuções por métrica);\n",
    "- Compare os resultados e discuta as vantagens/desvantagens de cada uma das métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I5t4Ap9bywpY"
   },
   "outputs": [],
   "source": [
    "# Seu codigo aqui\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIUUyGhs5-p0"
   },
   "source": [
    "## Parte 2 - Curva ROC e Teste de hipótese\n",
    "\n",
    "## Funções novas utilizadas no exercício\n",
    "\n",
    "- `pandas.Series.nunique()` ([link](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.nunique.html)): Conta quantidade de valores únicos de uma coluna. Útil para verificar se uma coluna é relevante ou não\n",
    "- `scipy.stats.ttest_ind()` ([link](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html#scipy.stats.ttest_ind)): Calcula o teste t para duas amostras independentes\n",
    "- `sklearn.metrics.plot_roc_curve()` ([link](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_roc_curve.html)): Plota a curva ROC de um classificador dado um conjunto de input e alvo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CjxiImds6u9L"
   },
   "source": [
    "### Questão 01.\n",
    "\n",
    "Faça a exploração dos dados. Isto é, carregue, substitua valores faltantes, padronize os dados, etc. Faça também a seleção dos atributos que achar mais relevantes. \n",
    "\n",
    " - Dica: Utilize a função `nunique()` durante a exploração dos dados. Você pode utilizar o \"bom senso\" (além de outras ferramentas é claro) na hora de escolher qual atributo do conjunto manter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 481,
     "status": "ok",
     "timestamp": 1622040616571,
     "user": {
      "displayName": "Guilherme Nardari",
      "photoUrl": "",
      "userId": "04838521268930040889"
     },
     "user_tz": 180
    },
    "id": "jTS4V3jN590F",
    "outputId": "8b285e9e-2a31-47d5-8255-a1a761828850"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"titanic.csv\")\n",
    "# df.info()\n",
    "df = df.drop(columns=[\"Name\", \"Ticket\", \"Cabin\"])\n",
    "df.isna().sum()\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KSPxl7Q-y4Mz"
   },
   "outputs": [],
   "source": [
    "# Etapas de pré-processamento que podem ser feitas antes de separar em treino/teste vão aqui\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "# 'PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
    "#        'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked\n",
    "\n",
    "features_categoricos = [ 'Sex', 'SibSp', 'Parch', 'Embarked']\n",
    "features_numericos = ['PassengerId', 'Survived', 'Pclass', 'Age',  'Fare']\n",
    "\n",
    "\n",
    "def transformar_base(df, features_numericos, features_categoricos):\n",
    "    \n",
    "    # Criando os pipelines\n",
    "\n",
    "    pipeline_numerico = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean'))\n",
    "    ]) #('scaler', StandardScaler())\n",
    "    \n",
    "    pipeline_categorico = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent'))\n",
    "        ]) #('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "\n",
    "    # Criando a transformação do conjunto de dados:\n",
    "    transformacao = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('transformacao numerica', pipeline_numerico, features_numericos),\n",
    "            ('transformacao categorica', pipeline_categorico, features_categoricos),        \n",
    "        ])\n",
    "    # Aplicando a transformação no dataset:\n",
    "    dados_transformados = pd.DataFrame(transformacao.fit_transform(df))\n",
    "    \n",
    "    \n",
    "    return dados_transformados\n",
    "\n",
    "dados_transformados = pd.DataFrame(transformar_base(df, features_numericos, features_categoricos))\n",
    "dados_transformados = dados_transformados.iloc[: , 1:]\n",
    "dados_transformados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_transformados.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h5rioqXl64_g"
   },
   "outputs": [],
   "source": [
    "# Etapas de pré-processamento que serão feitas após os dados estarem divididos vão na função\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Balanceando no conjunto de treino \n",
    "\n",
    "classe_0 = dados_transformados[dados_transformados[1] == 0]\n",
    "classe_1 = dados_transformados[dados_transformados[1] == 1]\n",
    "df_new = pd.concat([classe_0, classe_1.sample(n=207, replace=True)])\n",
    "\n",
    "x_treino, x_teste, y_treino, y_teste = train_test_split(df_new.loc[:, df_new.columns != 1], \n",
    "                                                                    df_new[1],\n",
    "                                                        test_size = 0.3,random_state = 0)\n",
    "\n",
    "x_treino.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9MVpvgX7Z0u"
   },
   "source": [
    "### Questão 02.\n",
    "\n",
    "Separe o conjunto de dados de maneira estratificada (através do parâmetro `stratify` da função `train_test_split`) em 20% para teste e 80% para treino. Depois plote a curva ROC (`sklearn.metrics.plot_roc_curve`) para **todos** os classificadores (no mesmo gráfico). \n",
    "\n",
    "Os melhores classificadores da questão anterior também apresentaram melhor desempenho na curva ROC? O que pode ter ocorrido? Teste diferentes valores de `random_state` na função `train_test_split` e observe o comportamento das curvas.\n",
    "\n",
    "\n",
    "*   Dica: Para plotar múltiplas curvas ROC no mesmo gráfico, defina uma figura com `fig, ax = plt.subplots()` e passe `ax` como parâmetro da função `plot_roc_curve`. Não se esqueça de passar também o nome do classificador para que o seu gráfico fique mais fácil de interpretar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oPHjrIF__aoa"
   },
   "outputs": [],
   "source": [
    "# Separar conjunto e pré processamento\n",
    "df_novo = dados_transformados.copy(deep=True)\n",
    "y = df_novo[1]\n",
    "del df_novo[1]\n",
    "X = df_novo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_novo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BUId5syM_BQD"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size = 0.2,\n",
    "                                                    random_state = 0)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0XKyiyeM7WTB"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import datasets, metrics\n",
    "import numpy as np\n",
    "\n",
    "classificadores = {\n",
    "  \"Perceptron\" : {\"modelo\": Perceptron(), \"scores\": []},\n",
    "  \"Multi-Layer Perceptron (15,)\" : {\"modelo\": MLPClassifier(random_state=1, hidden_layer_sizes=(15,), max_iter=2000), \"scores\": []},\n",
    "  \"Árvore Decisão Critério Gini\" : {\"modelo\": DecisionTreeClassifier(criterion='gini'), \"scores\": []},  \n",
    "  \"KNN k=5\" : {\"modelo\": KNeighborsClassifier(n_neighbors=5), \"scores\": []}\n",
    "}\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [15, 10]\n",
    "fig, ax = plt.subplots()\n",
    "for classificador_name in classificadores:  \n",
    "    \n",
    "    print( classificadores[classificador_name][\"modelo\"])\n",
    "    classificadores[classificador_name][\"modelo\"].fit(X=X_train, y=y_train) # 2) Fit\n",
    "\n",
    "    # valores preditos pelo classificador\n",
    "    y_pred = classificadores[classificador_name][\"modelo\"].predict(X_test)  # 3) Predict\n",
    "    y_test = np.array(y_test)\n",
    "    conf = cm(y_test, y_pred)\n",
    "\n",
    "    results[c_name][score_name].append(score_fn(conf))\n",
    "\n",
    "\n",
    " # 4) Avaliação com plot_roc_curve\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "klIBW1XGGez2"
   },
   "source": [
    "### Questão 03.\n",
    "\n",
    "Implemente o 10-Fold Cross Validation (pode usar o scikit) com os dois melhores classificadores de acordo com a curva ROC e guarde a acurácia de cada fold na chave 'scores' do dicionário de classificadores.\n",
    "- Sugestão: utilize o gabarito do Ex7 como referência para implementação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YBpcAFP4IHN0"
   },
   "outputs": [],
   "source": [
    "# Seu código aqui\n",
    "# Lembre-se do pré-processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q79VU9FK7Qr4"
   },
   "source": [
    "### Questão 04.\n",
    "\n",
    "Verifique se há diferença estatística significante entre suas acurácias da questão anterior utilizando o teste T (`scipy.stats.ttest_ind`). Considere que há diferença significante se p <= 0.05 (rejeita-se a hipótese nula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qWpS4QiH7TqG"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "# Seu código aqui"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Ex6.ipynb",
   "provenance": [
    {
     "file_id": "1rDUjv2mXomJnKX-xBoU6jWw25p52b0vA",
     "timestamp": 1591284005613
    },
    {
     "file_id": "1vh_baUGNIgEvcwczP8WSdv4Mkhb3aVrC",
     "timestamp": 1591283811719
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
